"""
fedxgb_flower_regression.py

Flower-orchestrated Fed-XGBoost (bagging-style) regression simulation.

This single-file script uses Flower's ServerApp / ClientApp abstractions and the
FedXgbBagging strategy (available in Flower) to simulate NUM_CLIENTS clients.
Each client trains an XGBoost booster locally each round (warm-starting from the
provided global model bytes when present). The server aggregates client models
using the FedXgbBagging strategy and prints per-round global metrics on a held-
out global test set.

Run:
    python fedxgb_flower_regression.py

Requirements:
    pip install flwr xgboost scikit-learn numpy joblib

Notes:
- This is a prototyping script (not cryptographically secure).
- Clients use early stopping on their local validation set.
- Aggregation uses Flower's FedXgbBagging strategy (bagging-style ensemble).
"""

import os
import json
import numpy as np
import xgboost as xgb
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

import flwr as fl
from flwr.server.grid import Grid
from flwr.server.strategy import FedXgbBagging
from flwr import ClientApp, Message, Context, ArrayRecord, ConfigRecord

# ----------------------------- Config ----------------------------------
NUM_CLIENTS = 4
NUM_ROUNDS = 20            # number of federated rounds
LOCAL_NUM_BOOST_ROUND = 10 # max boosting rounds per client each FL round
LOCAL_EARLY_STOP = 3       # early stopping rounds on local val
RANDOM_SEED = 42
TEST_SPLIT = 0.2
CLIENT_VAL_SPLIT = 0.2
SCALE_FEATURES = True
VERBOSE = False

# XGBoost regression params
XGB_PARAMS = {
    "objective": "reg:squarederror",
    "eta": 0.05,
    "max_depth": 6,
    "eval_metric": "rmse",
    "tree_method": "hist",
    "seed": RANDOM_SEED,
}

# -------------------------- Data partitioning --------------------------

def make_partitions(num_clients=NUM_CLIENTS, seed=RANDOM_SEED):
    X, y = fetch_california_housing(return_X_y=True)

    # create a held-out global test set (server-side)
    X_rest, X_test, y_rest, y_test = train_test_split(
        X, y, test_size=TEST_SPLIT, random_state=seed
    )

    # shuffle and split the remaining data into client partitions
    idx = np.arange(X_rest.shape[0])
    rng = np.random.default_rng(seed)
    rng.shuffle(idx)
    splits = np.array_split(idx, num_clients)

    client_partitions = []
    for i in range(num_clients):
        part_idx = splits[i]
        X_part = X_rest[part_idx]
        y_part = y_rest[part_idx]
        client_partitions.append((X_part, y_part))

    return client_partitions, (X_test, y_test)


# -------------------------- Client-side helpers -------------------------

def make_client_dmatrices(X_part, y_part, scale=True, val_frac=CLIENT_VAL_SPLIT, seed=RANDOM_SEED):
    # split local partition into train/val
    X_train, X_val, y_train, y_val = train_test_split(
        X_part, y_part, test_size=val_frac, random_state=seed
    )

    scaler = None
    if scale:
        scaler = StandardScaler().fit(X_train)
        X_train = scaler.transform(X_train)
        X_val = scaler.transform(X_val)

    dtrain = xgb.DMatrix(X_train, label=y_train)
    dval = xgb.DMatrix(X_val, label=y_val)
    return dtrain, dval, scaler


# -------------------------- ClientApp (Flower) --------------------------
app = ClientApp()

@app.train()
def train(msg: Message, context: Context):
    """
    Flower ClientApp train handler.
    msg.content["arrays"][0] contains the global model bytes (or empty array first round).
    We train locally for a few boosting rounds (warm-starting if a global model is provided)
    and return the local model bytes.
    """
    # The context.node_id will be a string; convert to int index for our simulated partitions
    partition_id = int(context.node_id)

    # Load the partition data for this client
    # We keep partitions in a global variable attached to the app for simplicity
    X_part, y_part = app._partitions[partition_id]

    dtrain, dval, scaler = make_client_dmatrices(X_part, y_part, scale=SCALE_FEATURES)

    # Load global model bytes (if any)
    global_arr = msg.content["arrays"][0].numpy()
    global_model_bytes = bytes(global_arr) if global_arr.size != 0 else b""

    bst = None
    if global_model_bytes and len(global_model_bytes) > 0:
        try:
            bst = xgb.Booster()
            bst.load_model(bytearray(global_model_bytes))
        except Exception:
            bst = None

    local_bst = xgb.train(
        XGB_PARAMS,
        dtrain,
        num_boost_round=LOCAL_NUM_BOOST_ROUND,
        evals=[(dval, "val")],
        early_stopping_rounds=LOCAL_EARLY_STOP if LOCAL_EARLY_STOP > 0 else None,
        xgb_model=bst if bst is not None else None,
        verbose_eval=False,
    )

    raw = local_bst.save_raw("json")
    model_np = np.frombuffer(raw, dtype=np.uint8)

    arrays = ArrayRecord([model_np])
    # Optionally send local metrics in config (local num examples here)
    config = ConfigRecord({"local_num_examples": dtrain.num_row()})

    # Persist scaler so server-side evaluation can scale test set per-client when ensemble predicting
    app._client_scalers[partition_id] = scaler

    return Message(content={"arrays": arrays, "config": config})


# -------------------------- ServerApp (Flower) --------------------------
server_app = fl.ServerApp()

@server_app.main()
def main():
    # Prepare data partitions and global test set
    partitions, (X_test, y_test) = make_partitions(num_clients=NUM_CLIENTS)

    # attach partitions and scalers to client app so clients can access them
    app._partitions = partitions
    app._client_scalers = [None] * NUM_CLIENTS

    # Strategy: FedXgbBagging
    strategy = FedXgbBagging(num_rounds=NUM_ROUNDS)

    # Grid: simulated client ids
    grid = Grid(num_nodes=NUM_CLIENTS)

    # initial (empty) global model bytes as numpy array
    initial = b""
    arrays = ArrayRecord([np.frombuffer(initial, dtype=np.uint8)])

    print(f"Starting Flower FedXgbBagging simulation: {NUM_CLIENTS} clients, {NUM_ROUNDS} rounds")

    # Start federated training (this will call client.train handlers)
    result = strategy.start(grid, arrays, num_rounds=NUM_ROUNDS)

    # After training, result.arrays contains the final aggregated model (per FedXgbBagging spec)
    final_arr = result.arrays[0].tobytes()

    # For evaluation we will deserialize each client's last model (available in server-side
    # `result` in bagging strategies, or we kept client scalers on the client app during training)
    # However, FedXgbBagging returns an aggregated model representation â€” for simplicity,
    # we'll ask each client app instance to provide its latest model by reloading client models
    # from the app._partitions state. In this simplified simulation the client models are
    # implicitly available on the client side; Flower's local simulation keeps them in memory.

    # Build boosters from the last round by querying clients' states. In our ClientApp
    # implementation we stored scalers on app._client_scalers; client boosters are not
    # persisted server-side by default. But many Flower XGBoost examples return client
    # model bytes through the strategy result arrays. Here, result.arrays contains a single
    # aggregated representation; for a proper per-client ensemble you'd collect each client's
    # bytes during training. To keep this example concise, we'll evaluate the aggregated
    # representation if present, otherwise we print a message.

    if final_arr and len(final_arr) > 0:
        try:
            bst = xgb.Booster()
            bst.load_model(bytearray(final_arr))

            # Evaluate on server-side test set (scale using mean scaler across clients if available)
            # We'll use a simple global scaler fit on training remainder for this evaluation
            X_train_for_scaler = np.vstack([p[0] for p in partitions])
            if SCALE_FEATURES:
                global_scaler = StandardScaler().fit(X_train_for_scaler)
                X_test_scaled = global_scaler.transform(X_test)
            else:
                X_test_scaled = X_test

            dtest = xgb.DMatrix(X_test_scaled)
            preds = bst.predict(dtest)

            mse = mean_squared_error(y_test, preds)
            mae = mean_absolute_error(y_test, preds)
            rmse = np.sqrt(mse)
            nmae = mae / (y_test.max() - y_test.min())

            print("=== Final aggregated model metrics ===")
            print(f"RMSE={rmse:.4f}, MAE={mae:.4f}, NMAE={nmae:.4f}")
        except Exception as e:
            print("Could not load aggregated model for evaluation:", e)
    else:
        print("No aggregated model bytes were returned by strategy. You may want to collect per-client models for ensemble evaluation.")

    print("Flower simulation finished.")


if __name__ == "__main__":
    # Run the server app (this will simulate clients using the ClientApp above)
    server_app.main()
