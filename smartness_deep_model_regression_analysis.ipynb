{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c3ba33b-3279-4727-b7f1-e8f0d481c211",
   "metadata": {},
   "source": [
    "# This notebook contains training step and analysis using nmae result..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52550860-12aa-474a-bebe-672f9b63b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e1df84-6caf-4898-959a-c7b556fdf533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Mean Absolute Error\n",
    "def nmae(y_pred, y_test):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mean_true = np.mean(np.abs(y_test))\n",
    "    return (mae / mean_true)\n",
    "\n",
    "def normalized_mean_absolute_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Normalized Mean Absolute Error (NMAE).\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Ground truth (correct) target values.\n",
    "        y_pred (array-like): Estimated target values.\n",
    "\n",
    "    Returns:\n",
    "        float: The Normalized Mean Absolute Error.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"y_true and y_pred must have the same length.\")\n",
    "\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "    # Calculate the range of actual values\n",
    "    y_range = np.max(y_true) - np.min(y_true)\n",
    "\n",
    "    # Avoid division by zero if the range is zero\n",
    "    if y_range == 0:\n",
    "        return 0.0 if mae == 0 else np.inf\n",
    "    else:\n",
    "        nmae = mae / y_range\n",
    "        return nmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "130f1eb1-ec08-4c7d-883b-f5305b2ee774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7803, 1864)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>container_blkio_device_usage_total_0</th>\n",
       "      <th>container_blkio_device_usage_total_1</th>\n",
       "      <th>container_blkio_device_usage_total_2</th>\n",
       "      <th>container_blkio_device_usage_total_3</th>\n",
       "      <th>container_blkio_device_usage_total_4</th>\n",
       "      <th>container_blkio_device_usage_total_5</th>\n",
       "      <th>container_blkio_device_usage_total_6</th>\n",
       "      <th>container_blkio_device_usage_total_7</th>\n",
       "      <th>container_blkio_device_usage_total_8</th>\n",
       "      <th>...</th>\n",
       "      <th>network_transmit_bytes_per_container_35</th>\n",
       "      <th>network_transmit_bytes_per_container_36</th>\n",
       "      <th>network_transmit_bytes_per_container_37</th>\n",
       "      <th>network_transmit_bytes_per_container_38</th>\n",
       "      <th>network_transmit_bytes_per_container_39</th>\n",
       "      <th>network_transmit_bytes_per_container_40</th>\n",
       "      <th>network_transmit_bytes_per_container_41</th>\n",
       "      <th>network_transmit_bytes_per_container_42</th>\n",
       "      <th>network_transmit_bytes_per_container_43</th>\n",
       "      <th>network_transmit_bytes_per_container_44</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1762718666</td>\n",
       "      <td>124854272</td>\n",
       "      <td>523153408</td>\n",
       "      <td>124854272</td>\n",
       "      <td>504750080</td>\n",
       "      <td>22294528</td>\n",
       "      <td>18027241472</td>\n",
       "      <td>22294528</td>\n",
       "      <td>17279209472</td>\n",
       "      <td>125186048</td>\n",
       "      <td>...</td>\n",
       "      <td>12989.561587</td>\n",
       "      <td>0</td>\n",
       "      <td>91995.257453</td>\n",
       "      <td>0</td>\n",
       "      <td>13904.017857</td>\n",
       "      <td>0</td>\n",
       "      <td>94496.974755</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1762718667</td>\n",
       "      <td>124854272</td>\n",
       "      <td>523153408</td>\n",
       "      <td>124854272</td>\n",
       "      <td>504750080</td>\n",
       "      <td>22294528</td>\n",
       "      <td>18027241472</td>\n",
       "      <td>22294528</td>\n",
       "      <td>17279209472</td>\n",
       "      <td>125186048</td>\n",
       "      <td>...</td>\n",
       "      <td>12841.486068</td>\n",
       "      <td>0</td>\n",
       "      <td>93890.433700</td>\n",
       "      <td>0</td>\n",
       "      <td>14582.299227</td>\n",
       "      <td>0</td>\n",
       "      <td>104701.936976</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1762718668</td>\n",
       "      <td>124854272</td>\n",
       "      <td>523153408</td>\n",
       "      <td>124854272</td>\n",
       "      <td>504750080</td>\n",
       "      <td>22294528</td>\n",
       "      <td>18027241472</td>\n",
       "      <td>22294528</td>\n",
       "      <td>17279209472</td>\n",
       "      <td>125186048</td>\n",
       "      <td>...</td>\n",
       "      <td>13765.763274</td>\n",
       "      <td>0</td>\n",
       "      <td>74584.501237</td>\n",
       "      <td>0</td>\n",
       "      <td>12281.804734</td>\n",
       "      <td>0</td>\n",
       "      <td>101769.951293</td>\n",
       "      <td>0</td>\n",
       "      <td>12604.872585</td>\n",
       "      <td>430005.600672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1762718669</td>\n",
       "      <td>124854272</td>\n",
       "      <td>523153408</td>\n",
       "      <td>124854272</td>\n",
       "      <td>504750080</td>\n",
       "      <td>22294528</td>\n",
       "      <td>18027241472</td>\n",
       "      <td>22294528</td>\n",
       "      <td>17279209472</td>\n",
       "      <td>125186048</td>\n",
       "      <td>...</td>\n",
       "      <td>11904.655612</td>\n",
       "      <td>0</td>\n",
       "      <td>97924.033523</td>\n",
       "      <td>0</td>\n",
       "      <td>12256.643701</td>\n",
       "      <td>0</td>\n",
       "      <td>94527.552886</td>\n",
       "      <td>0</td>\n",
       "      <td>10402.588398</td>\n",
       "      <td>354876.357754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1762718670</td>\n",
       "      <td>124854272</td>\n",
       "      <td>523153408</td>\n",
       "      <td>124854272</td>\n",
       "      <td>504750080</td>\n",
       "      <td>22294528</td>\n",
       "      <td>18027241472</td>\n",
       "      <td>22294528</td>\n",
       "      <td>17279209472</td>\n",
       "      <td>125186048</td>\n",
       "      <td>...</td>\n",
       "      <td>7763.962677</td>\n",
       "      <td>0</td>\n",
       "      <td>97924.033523</td>\n",
       "      <td>0</td>\n",
       "      <td>10511.954993</td>\n",
       "      <td>0</td>\n",
       "      <td>80978.202448</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1864 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  container_blkio_device_usage_total_0  \\\n",
       "0  1762718666                             124854272   \n",
       "1  1762718667                             124854272   \n",
       "2  1762718668                             124854272   \n",
       "3  1762718669                             124854272   \n",
       "4  1762718670                             124854272   \n",
       "\n",
       "   container_blkio_device_usage_total_1  container_blkio_device_usage_total_2  \\\n",
       "0                             523153408                             124854272   \n",
       "1                             523153408                             124854272   \n",
       "2                             523153408                             124854272   \n",
       "3                             523153408                             124854272   \n",
       "4                             523153408                             124854272   \n",
       "\n",
       "   container_blkio_device_usage_total_3  container_blkio_device_usage_total_4  \\\n",
       "0                             504750080                              22294528   \n",
       "1                             504750080                              22294528   \n",
       "2                             504750080                              22294528   \n",
       "3                             504750080                              22294528   \n",
       "4                             504750080                              22294528   \n",
       "\n",
       "   container_blkio_device_usage_total_5  container_blkio_device_usage_total_6  \\\n",
       "0                           18027241472                              22294528   \n",
       "1                           18027241472                              22294528   \n",
       "2                           18027241472                              22294528   \n",
       "3                           18027241472                              22294528   \n",
       "4                           18027241472                              22294528   \n",
       "\n",
       "   container_blkio_device_usage_total_7  container_blkio_device_usage_total_8  \\\n",
       "0                           17279209472                             125186048   \n",
       "1                           17279209472                             125186048   \n",
       "2                           17279209472                             125186048   \n",
       "3                           17279209472                             125186048   \n",
       "4                           17279209472                             125186048   \n",
       "\n",
       "   ...  network_transmit_bytes_per_container_35  \\\n",
       "0  ...                             12989.561587   \n",
       "1  ...                             12841.486068   \n",
       "2  ...                             13765.763274   \n",
       "3  ...                             11904.655612   \n",
       "4  ...                              7763.962677   \n",
       "\n",
       "   network_transmit_bytes_per_container_36  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "\n",
       "   network_transmit_bytes_per_container_37  \\\n",
       "0                             91995.257453   \n",
       "1                             93890.433700   \n",
       "2                             74584.501237   \n",
       "3                             97924.033523   \n",
       "4                             97924.033523   \n",
       "\n",
       "   network_transmit_bytes_per_container_38  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "\n",
       "   network_transmit_bytes_per_container_39  \\\n",
       "0                             13904.017857   \n",
       "1                             14582.299227   \n",
       "2                             12281.804734   \n",
       "3                             12256.643701   \n",
       "4                             10511.954993   \n",
       "\n",
       "   network_transmit_bytes_per_container_40  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "\n",
       "   network_transmit_bytes_per_container_41  \\\n",
       "0                             94496.974755   \n",
       "1                            104701.936976   \n",
       "2                            101769.951293   \n",
       "3                             94527.552886   \n",
       "4                             80978.202448   \n",
       "\n",
       "   network_transmit_bytes_per_container_42  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "\n",
       "   network_transmit_bytes_per_container_43  \\\n",
       "0                                 0.000000   \n",
       "1                                 0.000000   \n",
       "2                             12604.872585   \n",
       "3                             10402.588398   \n",
       "4                                 0.000000   \n",
       "\n",
       "   network_transmit_bytes_per_container_44  \n",
       "0                                 0.000000  \n",
       "1                                 0.000000  \n",
       "2                            430005.600672  \n",
       "3                            354876.357754  \n",
       "4                                 0.000000  \n",
       "\n",
       "[5 rows x 1864 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load write dataset\n",
    "x_ds_c500 = pd.read_csv('datasets/exp60c_2h/t500/prometheus_metrics_wide.csv', low_memory=True).apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "print(x_ds_c500.shape)\n",
    "x_ds_c500.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61052d86-5004-43c0-85b8-e98642707b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7803, 45)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>queries_num</th>\n",
       "      <th>queries_requested</th>\n",
       "      <th>errors_occurred</th>\n",
       "      <th>iter_errors_occurred</th>\n",
       "      <th>average_latency</th>\n",
       "      <th>99_9_latency_percentile</th>\n",
       "      <th>mean_rate</th>\n",
       "      <th>one_minute_rate</th>\n",
       "      <th>five_minute_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>w_min</th>\n",
       "      <th>w_max</th>\n",
       "      <th>w_mean</th>\n",
       "      <th>w_std_dev</th>\n",
       "      <th>w_median</th>\n",
       "      <th>w_75th_percentile</th>\n",
       "      <th>w_95th_percentile</th>\n",
       "      <th>w_98th_percentile</th>\n",
       "      <th>w_99th_percentile</th>\n",
       "      <th>w_99_9th_percentile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1762718666</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1762718667</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>1.236033</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1762718668</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.105083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.930949</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.85</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1762718669</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.156597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4.208333</td>\n",
       "      <td>0.779028</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.45</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1762718670</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.207441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4.161290</td>\n",
       "      <td>0.687836</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.10</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  queries_num  queries_requested  errors_occurred  \\\n",
       "0  1762718666            0                  0                0   \n",
       "1  1762718667            9                  0                0   \n",
       "2  1762718668           16                  0                0   \n",
       "3  1762718669           24                  0                0   \n",
       "4  1762718670           32                  0                0   \n",
       "\n",
       "   iter_errors_occurred  average_latency  99_9_latency_percentile  mean_rate  \\\n",
       "0                     0                0                        0   0.000000   \n",
       "1                     0                4                        1   0.059500   \n",
       "2                     0                4                        1   0.105083   \n",
       "3                     0                4                        1   0.156597   \n",
       "4                     0                4                        1   0.207441   \n",
       "\n",
       "   one_minute_rate  five_minute_rate  ...  w_min  w_max    w_mean  w_std_dev  \\\n",
       "0              0.0               0.0  ...      0      0  0.000000   0.000000   \n",
       "1              0.0               0.0  ...      3      7  4.444444   1.236033   \n",
       "2              0.0               0.0  ...      3      7  4.250000   0.930949   \n",
       "3              0.0               0.0  ...      3      7  4.208333   0.779028   \n",
       "4              0.0               0.0  ...      3      7  4.161290   0.687836   \n",
       "\n",
       "   w_median  w_75th_percentile  w_95th_percentile  w_98th_percentile  \\\n",
       "0       0.0           0.000000               0.00                0.0   \n",
       "1       4.0           4.666667               7.00                7.0   \n",
       "2       4.0           4.000000               6.85                7.0   \n",
       "3       4.0           4.000000               6.45                7.0   \n",
       "4       4.0           4.000000               6.10                7.0   \n",
       "\n",
       "   w_99th_percentile  w_99_9th_percentile  \n",
       "0                0.0                    0  \n",
       "1                7.0                    7  \n",
       "2                7.0                    7  \n",
       "3                7.0                    7  \n",
       "4                7.0                    7  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ds_c500 = pd.read_csv('datasets/exp60c_2h/t500/20251109_200426169_w.csv', low_memory=True).apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "print(y_ds_c500.shape)\n",
    "y_ds_c500.head(5)\n",
    "\n",
    "# y_ds_c500_w_99th_percentile = y_ds_c500[['w_99th_percentile']].copy()\n",
    "# print(y_ds_c500_w_99th_percentile.shape)\n",
    "# y_ds_c500_w_99th_percentile.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905c5398-a37d-4aa0-ae8b-b08403af1f9a",
   "metadata": {},
   "source": [
    "# Random Forest training using all features\n",
    "\n",
    "## train 70%, test 30%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd63b590-c475-4b52-973e-0c08a55e0537",
   "metadata": {},
   "source": [
    "## T500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dba3c0-236d-40e9-8197-4df3235b12ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_scaler = MinMaxScaler()\n",
    "x_ds_scaled = X_scaler.fit_transform(x_ds_c500)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_ds_c500_w_99th_percentile = y_ds_c500[['w_99th_percentile']].copy()\n",
    "y_ds_scaled = y_scaler.fit_transform(y_ds_c500_w_99th_percentile)\n",
    "\n",
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  x_ds_scaled, y_ds_scaled, test_size=0.30, random_state=42)\n",
    "\n",
    "class Data(Dataset):\n",
    "  '''Dataset Class to store the samples and their corresponding labels, \n",
    "  and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "  '''\n",
    "\n",
    "  def __init__(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "\n",
    "    # need to convert float64 to float32 else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Double but found Float\n",
    "    self.X = torch.from_numpy(X.astype(np.float32))\n",
    "    self.y = torch.from_numpy(y.astype(np.float32))\n",
    "    self.len = self.X.shape[0]\n",
    "  \n",
    "  def __getitem__(self, index: int) -> tuple:\n",
    "    return self.X[index], self.y[index]\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    return self.len\n",
    "  \n",
    "# Generate the training dataset\n",
    "traindata = Data(X_train, y_train)\n",
    "\n",
    "batch_size = 64\n",
    "# tells the data loader instance how many sub-processes to use for data loading\n",
    "# if the num_worker is zero (default) the GPU has to weight for CPU to load data\n",
    "# Theoretically, greater the num_workers, \n",
    "# more efficiently the CPU load data and less the GPU has to wait\n",
    "num_workers = 2\n",
    "\n",
    "# Load the training data into data loader with the \n",
    "# respective batch_size and num_workers\n",
    "trainloader = DataLoader(traindata, batch_size=batch_size, \n",
    "                         shuffle=True, num_workers=num_workers)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "  '''Linear Regression Model\n",
    "  '''\n",
    "\n",
    "  def __init__(self, input_dim: int, hidden_dim: int, output_dim: int) -> None:\n",
    "    '''The network has 4 layers\n",
    "         - input layer\n",
    "         - hidden layer\n",
    "         - hidden layer\n",
    "         - output layer\n",
    "    '''\n",
    "    super(LinearRegression, self).__init__()\n",
    "    self.input_to_hidden = nn.Linear(input_dim, hidden_dim)\n",
    "    self.hidden_layer_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.hidden_layer_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.hidden_to_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # no activation and no softmax at the end\n",
    "    x = self.input_to_hidden(x)\n",
    "    x = self.hidden_layer_1(x)\n",
    "    x = self.hidden_layer_2(x)\n",
    "    x = self.hidden_to_output(x)\n",
    "    return x\n",
    "\n",
    "# number of features (len of X cols)\n",
    "input_dim = X_train.shape[1]\n",
    "# number of hidden layers\n",
    "hidden_layers = 50\n",
    "# output dimension is 1 because of linear regression\n",
    "output_dim = 1\n",
    "# initiate the linear regression model\n",
    "model = LinearRegression(input_dim, hidden_layers, output_dim)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "048af645-8a79-49e9-bcf2-31f386122771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand. Forest Training time: 44.28541s\n",
      "Full dataset | 99th_percentile -> Rand. Forest NMAE: 0.27%\n",
      "Rand. Forest Training time: 56.095947s\n",
      "Full dataset | d_99th_percentile -> Rand. Forest NMAE: 3.56%\n",
      "Rand. Forest Training time: 93.024478s\n",
      "Full dataset | w_99th_percentile -> Rand. Forest NMAE: 17.66%\n"
     ]
    }
   ],
   "source": [
    "# Full dataset - wihout normalization...\n",
    "\n",
    "random_forest_model = RandomForestRegressor(n_estimators=240, random_state=42, n_jobs=-1)\n",
    "\n",
    "# using 99th_percentile\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_ds_c500, y_ds_c500['99th_percentile'].to_numpy(), test_size=0.30, random_state=42)\n",
    "\n",
    "time = datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "random_forest_model.fit(x_train, y_train)\n",
    "print(f'Rand. Forest Training time: {(datetime.datetime.now(tz=datetime.timezone.utc) - time).total_seconds()}s')\n",
    "\n",
    "predicted = random_forest_model.predict(x_test)\n",
    "print(f'Full dataset | 99th_percentile -> Rand. Forest NMAE: {nmae(predicted, y_test):.2%}')\n",
    "\n",
    "# using d_99th_percentile\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_ds_c500, y_ds_c500['d_99th_percentile'].to_numpy(), test_size=0.30, random_state=42)\n",
    "\n",
    "time = datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "random_forest_model.fit(x_train, y_train)\n",
    "print(f'Rand. Forest Training time: {(datetime.datetime.now(tz=datetime.timezone.utc) - time).total_seconds()}s')\n",
    "\n",
    "predicted = random_forest_model.predict(x_test)\n",
    "print(f'Full dataset | d_99th_percentile -> Rand. Forest NMAE: {nmae(predicted, y_test):.2%}')\n",
    "\n",
    "# using w_99th_percentile\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_ds_c500, y_ds_c500['w_99th_percentile'].to_numpy(), test_size=0.30, random_state=42)\n",
    "\n",
    "time = datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "random_forest_model.fit(x_train, y_train)\n",
    "print(f'Rand. Forest Training time: {(datetime.datetime.now(tz=datetime.timezone.utc) - time).total_seconds()}s')\n",
    "\n",
    "predicted = random_forest_model.predict(x_test)\n",
    "print(f'Full dataset | w_99th_percentile -> Rand. Forest NMAE: {nmae(predicted, y_test):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55a828ce-690f-4988-b978-7a2a81313788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand. Forest Training time: 48.580744s\n",
      "Full dataset | 99th_percentile -> Rand. Forest NMAE: 0.27%\n",
      "Rand. Forest Training time: 59.981796s\n",
      "Full dataset | d_99th_percentile -> Rand. Forest NMAE: 3.61%\n",
      "Rand. Forest Training time: 97.792044s\n",
      "Full dataset | w_99th_percentile -> Rand. Forest NMAE: 17.69%\n"
     ]
    }
   ],
   "source": [
    "# Full dataset, using pipeline to normalize features and target.\n",
    "\n",
    "# Define the feature pipeline\n",
    "feature_pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()), # Normalize input features\n",
    "])\n",
    "\n",
    "rf_regressor = RandomForestRegressor(n_estimators=240, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Combine with target scaling using TransformedTargetRegressor\n",
    "model = TransformedTargetRegressor(\n",
    "    regressor=Pipeline([\n",
    "        ('preprocess', feature_pipeline),\n",
    "        ('model', rf_regressor)\n",
    "    ]),\n",
    "    transformer=MinMaxScaler()  # Normalizes the target y\n",
    ")\n",
    "\n",
    "# using 99th_percentile\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_ds_c500, y_ds_c500['99th_percentile'].to_numpy(), test_size=0.30, random_state=42)\n",
    "\n",
    "time = datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "model.fit(x_train, y_train)\n",
    "print(f'Rand. Forest Training time: {(datetime.datetime.now(tz=datetime.timezone.utc) - time).total_seconds()}s')\n",
    "\n",
    "predicted = model.predict(x_test)\n",
    "print(f'Full dataset | 99th_percentile -> Rand. Forest NMAE: {nmae(predicted, y_test):.2%}')\n",
    "\n",
    "# using d_99th_percentile\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_ds_c500, y_ds_c500['d_99th_percentile'].to_numpy(), test_size=0.30, random_state=42)\n",
    "\n",
    "time = datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "model.fit(x_train, y_train)\n",
    "print(f'Rand. Forest Training time: {(datetime.datetime.now(tz=datetime.timezone.utc) - time).total_seconds()}s')\n",
    "\n",
    "predicted = model.predict(x_test)\n",
    "print(f'Full dataset | d_99th_percentile -> Rand. Forest NMAE: {nmae(predicted, y_test):.2%}')\n",
    "\n",
    "# using w_99th_percentile\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_ds_c500, y_ds_c500['w_99th_percentile'].to_numpy(), test_size=0.30, random_state=42)\n",
    "\n",
    "time = datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "model.fit(x_train, y_train)\n",
    "print(f'Rand. Forest Training time: {(datetime.datetime.now(tz=datetime.timezone.utc) - time).total_seconds()}s')\n",
    "\n",
    "predicted = model.predict(x_test)\n",
    "print(f'Full dataset | w_99th_percentile -> Rand. Forest NMAE: {nmae(predicted, y_test):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "080cce95-1c4b-4703-88d0-a11480cdf5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand. Forest Training time: 87.175224s\n",
      "Full dataset | 99th_percentile -> Rand. Forest NMAE: 0.44%\n",
      "Rand. Forest Training time: 117.375869s\n",
      "Full dataset | d_99th_percentile -> Rand. Forest NMAE: 3.74%\n",
      "Rand. Forest Training time: 118.866501s\n",
      "Full dataset | w_99th_percentile -> Rand. Forest NMAE: 16.17%\n"
     ]
    }
   ],
   "source": [
    "# Full dataset, using xgboost\n",
    "\n",
    "# 3. Initialize the XGBoost Regressor model\n",
    "# objective='reg:squarederror' is the default for regression, but explicitly setting it is good practice.\n",
    "# n_estimators controls the number of boosting rounds (trees).\n",
    "# random_state for reproducibility.\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8, random_state=42)\n",
    "\n",
    "# using 99th_percentile\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_ds_c500, y_ds_c500['99th_percentile'].to_numpy(), test_size=0.30, random_state=42)\n",
    "\n",
    "time = datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "model.fit(x_train, y_train)\n",
    "print(f'Rand. Forest Training time: {(datetime.datetime.now(tz=datetime.timezone.utc) - time).total_seconds()}s')\n",
    "\n",
    "predicted = model.predict(x_test)\n",
    "print(f'Full dataset | 99th_percentile -> Rand. Forest NMAE: {nmae(predicted, y_test):.2%}')\n",
    "\n",
    "# using d_99th_percentile\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_ds_c500, y_ds_c500['d_99th_percentile'].to_numpy(), test_size=0.30, random_state=42)\n",
    "\n",
    "time = datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "model.fit(x_train, y_train)\n",
    "print(f'Rand. Forest Training time: {(datetime.datetime.now(tz=datetime.timezone.utc) - time).total_seconds()}s')\n",
    "\n",
    "predicted = model.predict(x_test)\n",
    "print(f'Full dataset | d_99th_percentile -> Rand. Forest NMAE: {nmae(predicted, y_test):.2%}')\n",
    "\n",
    "# using w_99th_percentile\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_ds_c500, y_ds_c500['w_99th_percentile'].to_numpy(), test_size=0.30, random_state=42)\n",
    "\n",
    "time = datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "model.fit(x_train, y_train)\n",
    "print(f'Rand. Forest Training time: {(datetime.datetime.now(tz=datetime.timezone.utc) - time).total_seconds()}s')\n",
    "\n",
    "predicted = model.predict(x_test)\n",
    "print(f'Full dataset | w_99th_percentile -> Rand. Forest NMAE: {nmae(predicted, y_test):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47beee0e-5a92-41cc-b9ab-47fa20a6fcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7803, 1864)\n",
      "(7803, 1867)\n",
      "(6981, 1867)\n",
      "(6981, 3)\n",
      "(6981, 1864)\n",
      "Rand. Forest Training time: 14.45166s\n",
      "Full dataset | 99th_percentile -> Rand. Forest NMAE: 0.21%\n",
      "Rand. Forest Training time: 18.403955s\n",
      "Full dataset | d_99th_percentile -> Rand. Forest NMAE: 2.58%\n",
      "Rand. Forest Training time: 27.313033s\n",
      "Full dataset | w_99th_percentile -> Rand. Forest NMAE: 8.80%\n"
     ]
    }
   ],
   "source": [
    "# Dataset removing latencies > 800ms - wihout normalization...\n",
    "# Define the list of columns to be included in the new DataFrame\n",
    "selected_columns = ['timestamp', '99th_percentile', 'd_99th_percentile', 'w_99th_percentile']\n",
    "\n",
    "# Create the new DataFrame by selecting the specified columns\n",
    "y_ds_c500_filtered = y_ds_c500[selected_columns].copy()\n",
    "# print(y_ds_c500_filtered.shape)\n",
    "# y_ds_c500_filtered = y_ds_c500_filtered[(y_ds_c500_filtered['w_99th_percentile'] < 800.00)]\n",
    "# print(y_ds_c500_filtered.shape)\n",
    "\n",
    "# print(\"Min:\", y_ds_c500_filtered['w_99th_percentile'].min())\n",
    "# print(\"Max:\", y_ds_c500_filtered['w_99th_percentile'].max())\n",
    "\n",
    "print(x_ds_c500.shape)\n",
    "# Merge x_ds_c500 with y_ds_c500_filtered to add the 'Target' column\n",
    "x_ds_c500_merged = pd.merge(x_ds_c500, y_ds_c500_filtered, on='timestamp', how='left')\n",
    "print(x_ds_c500_merged.shape)\n",
    "\n",
    "# Filter target columns to less than 800ms\n",
    "x_ds_c500_filtered = x_ds_c500_merged[\n",
    "    (x_ds_c500_merged['99th_percentile'] < 100.00) & \n",
    "    (x_ds_c500_merged['d_99th_percentile'] < 100.00) & \n",
    "    (x_ds_c500_merged['w_99th_percentile'] < 100.00)\n",
    "]\n",
    "print(x_ds_c500_filtered.shape)\n",
    "\n",
    "# Create a new DataFrame with the columns to be dropped\n",
    "columns_to_drop = ['99th_percentile', 'd_99th_percentile', 'w_99th_percentile']\n",
    "y_ds_c500_filtered = x_ds_c500_filtered[columns_to_drop].copy()\n",
    "print(y_ds_c500_filtered.shape)\n",
    "\n",
    "# Drop the columns from the original DataFrame to create a new DataFrame without them\n",
    "x_ds_c500_filtered = x_ds_c500_filtered.drop(columns=columns_to_drop, axis=1)\n",
    "print(x_ds_c500_filtered.shape)\n",
    "\n",
    "random_forest_model = RandomForestRegressor(n_estimators=120, random_state=42, n_jobs=-1)\n",
    "\n",
    "# using 99th_percentile\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_ds_c500_filtered, y_ds_c500_filtered['99th_percentile'].to_numpy(), test_size=0.30, random_state=42)\n",
    "\n",
    "time = datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "random_forest_model.fit(x_train, y_train)\n",
    "print(f'Rand. Forest Training time: {(datetime.datetime.now(tz=datetime.timezone.utc) - time).total_seconds()}s')\n",
    "\n",
    "predicted = random_forest_model.predict(x_test)\n",
    "print(f'Full dataset | 99th_percentile -> Rand. Forest NMAE: {nmae(predicted, y_test):.2%}')\n",
    "\n",
    "# using d_99th_percentile\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_ds_c500_filtered, y_ds_c500_filtered['d_99th_percentile'].to_numpy(), test_size=0.30, random_state=42)\n",
    "\n",
    "time = datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "random_forest_model.fit(x_train, y_train)\n",
    "print(f'Rand. Forest Training time: {(datetime.datetime.now(tz=datetime.timezone.utc) - time).total_seconds()}s')\n",
    "\n",
    "predicted = random_forest_model.predict(x_test)\n",
    "print(f'Full dataset | d_99th_percentile -> Rand. Forest NMAE: {nmae(predicted, y_test):.2%}')\n",
    "\n",
    "# using w_99th_percentile\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_ds_c500_filtered, y_ds_c500_filtered['w_99th_percentile'].to_numpy(), test_size=0.30, random_state=42)\n",
    "\n",
    "time = datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "random_forest_model.fit(x_train, y_train)\n",
    "print(f'Rand. Forest Training time: {(datetime.datetime.now(tz=datetime.timezone.utc) - time).total_seconds()}s')\n",
    "\n",
    "predicted = random_forest_model.predict(x_test)\n",
    "print(f'Full dataset | w_99th_percentile -> Rand. Forest NMAE: {nmae(predicted, y_test):.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
